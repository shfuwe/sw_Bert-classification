{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import jieba\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from transformers import AdamW\n",
    "from transformers import BertModel,BertTokenizer\n",
    "www=[2.980574324324324, 1.0, 4.089223638470452, 9.748618784530386, 7.02988047808765, 2.204247345409119, 3.992081447963801, 8.889168765743072, 8.503614457831326, 11.80267558528428, 3.529, 4.433417085427136, 3.529, 3.529]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\"\"\"\n",
    "手动实现transformer.models.bert.BertForSequenceClassification()函数\n",
    "根据论文[How to Fine-Tune BERT for Text Classification（2019）](https://www.aclweb.org/anthology/P18-1031.pdf)\n",
    "在分类问题上，把最后四层进行concat然后maxpooling 输出的结果会比直接输出最后一层的要好\n",
    "这里进行实现测试\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "class bert_lr_last4layer_Config(nn.Module):\n",
    "    def __init__(self):\n",
    "        self.bert_path = \"../chinese-bert-wwm\"\n",
    "        self.config_path = \"../chinese-bert-wwm/config.json\"\n",
    "\n",
    "        # self.tokenizer = BertTokenizer.from_pretrained(self.bert_path)\n",
    "        self.hidden_size = 768\n",
    "        self.num_labels = type_num\n",
    "        # self.dropout_bertout = 0.2\n",
    "        self.dropout_bertout = 0.5\n",
    "        self.mytrainedmodel = \"../result/bert_clf_model.bin\"\n",
    "        \"\"\"\n",
    "        current loss: 0.4363991916179657 \t current acc: 0.8125\n",
    "        current loss: 0.1328232882924341 \t current acc: 0.9527363184079602\n",
    "        current loss: 0.11797185830000853 \t current acc: 0.9585411471321695\n",
    "        train loss:  0.11880445411248554 \t train acc: 0.9583704495516361\n",
    "        valid loss:  0.1511497257672476 \t valid acc: 0.9431549028896258\n",
    "        \"\"\"\n",
    "\n",
    "class bert_lr_last4layer(nn.Module):\n",
    "\n",
    "    def __init__(self,config):\n",
    "        super(bert_lr_last4layer, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(config.bert_path,config = config.config_path)\n",
    "        self.dropout_bertout = nn.Dropout(config.dropout_bertout)\n",
    "        self.num_labels = config.num_labels\n",
    "        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
    "        self.sigmoid=nn.Sigmoid() \n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        labels=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=True,\n",
    "        return_dict=None,\n",
    "    ):\n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        # outputs = outputs[2] # [1]是pooled的结果 # [3]是hidden_states 12层\n",
    "        hidden_states = outputs.hidden_states\n",
    "        nopooled_output = torch.cat((hidden_states[9],hidden_states[10],hidden_states[11],hidden_states[12]),1)\n",
    "        batch_size = nopooled_output.shape[0] # 32\n",
    "        # print(batch_size)\n",
    "        # print(nopooled_output.shape) # torch.Size([32, 400, 768])\n",
    "        kernel_hight = nopooled_output.shape[1]\n",
    "        pooled_output = F.max_pool2d(nopooled_output,kernel_size = (kernel_hight,1))\n",
    "        # print(pooled_output.shape) # torch.Size([32, 1, 768])\n",
    "\n",
    "        flatten = pooled_output.view(batch_size,-1)\n",
    "        # print(flatten.shape) # [32,768]\n",
    "\n",
    "        flattened_output = self.dropout_bertout(flatten)\n",
    "\n",
    "        logits = self.classifier(flattened_output)\n",
    "        logits=self.sigmoid(logits)\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            if self.num_labels == 1:\n",
    "                #  We are doing regression\n",
    "                loss_fct = nn.MSELoss()\n",
    "                loss = loss_fct(logits.view(-1), labels.view(-1))\n",
    "            else:\n",
    "                loss_fct = nn.CrossEntropyLoss()\n",
    "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (logits,) + outputs[2:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return loss,logits\n",
    "\n",
    "\n",
    "class Config(object):\n",
    "    def __init__(self):\n",
    "        self.config_dict = {\n",
    "            \"data_path\": {\n",
    "                # \"trainingSet_path\": \"../data/sentiment/sentiment.train0.data\",\n",
    "                # \"valSet_path\": \"../data/sentiment/sentiment.valid0.data\",\n",
    "                \"trainingSet_path\": trainingSet_path0,\n",
    "                \"valSet_path\": valSet_path0,\n",
    "                \"testingSet_path\": \"../data/sentiment/sentiment.test0.data\",\n",
    "                \"zeng_path\": zeng_path0,\n",
    "                \"bad_path\":bad_path0\n",
    "            },\n",
    "\n",
    "            \"BERT_path\": {\n",
    "                \"file_path\": '../chinese-bert-wwm/',\n",
    "                \"config_path\": '../chinese-bert-wwm/',\n",
    "                \"vocab_path\": '../chinese-bert-wwm/',\n",
    "            },\n",
    "\n",
    "            \"training_rule\": {\n",
    "                \"max_length\": 100,  # 输入序列长度，别超过512\n",
    "                \"hidden_dropout_prob\": 0.3,\n",
    "                \"num_labels\": type_num,  # 几分类个数\n",
    "                \"learning_rate\": 1e-5,\n",
    "                \"weight_decay\": 1e-2,\n",
    "                \"batch_size\": 16\n",
    "            },\n",
    "\n",
    "            \"result\": {\n",
    "                \"model_save_path\": '../result/bert_clf_model.bin',\n",
    "                \"config_save_path\": '../result/bert_clf_config.json',\n",
    "                \"vocab_save_path\": '../result/bert_clf_vocab.txt'\n",
    "            }\n",
    "        }\n",
    "\n",
    "    def get(self, section, name):\n",
    "        return self.config_dict[section][name]\n",
    "\n",
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, path_to_file):\n",
    "#         print(path_to_file)\n",
    "        self.dataset = pd.read_csv(path_to_file, sep=\"\\t\", names=[\"text\", \"label\"])\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.dataset.loc[idx, \"text\"]\n",
    "        label = self.dataset.loc[idx, \"label\"]\n",
    "        sample = {\"text\": text, \"label\": label}\n",
    "        # print(sample)\n",
    "        return sample\n",
    "\n",
    "def convert_text_to_ids(tokenizer, text, max_len=100):\n",
    "    if isinstance(text, str):\n",
    "        tokenized_text = tokenizer.encode_plus(text, max_length=max_len, add_special_tokens=True, truncation=True)\n",
    "        input_ids = tokenized_text[\"input_ids\"]\n",
    "        token_type_ids = tokenized_text[\"token_type_ids\"]\n",
    "    elif isinstance(text, list):\n",
    "        input_ids = []\n",
    "        token_type_ids = []\n",
    "        for t in text:\n",
    "            tokenized_text = tokenizer.encode_plus(t, max_length=max_len, add_special_tokens=True, truncation=True)\n",
    "            input_ids.append(tokenized_text[\"input_ids\"])\n",
    "            token_type_ids.append(tokenized_text[\"token_type_ids\"])\n",
    "    else:\n",
    "        print(\"Unexpected input\")\n",
    "    return input_ids, token_type_ids\n",
    "\n",
    "def seq_padding(tokenizer, X):\n",
    "    pad_id = tokenizer.convert_tokens_to_ids(\"[PAD]\")\n",
    "    if len(X) <= 1:\n",
    "        return torch.tensor(X)\n",
    "    L = [len(x) for x in X]\n",
    "    ML = max(L)\n",
    "    X = torch.Tensor([x + [pad_id] * (ML - len(x)) if len(x) < ML else x for x in X])\n",
    "    return X\n",
    "        \n",
    "class transformers_bert_binary_classification(object):\n",
    "    def __init__(self):\n",
    "        self.config = Config()\n",
    "        self.device_setup()\n",
    "        self.sigmoid=nn.Sigmoid()\n",
    "\n",
    "    def device_setup(self):\n",
    "        \"\"\"\n",
    "        设备配置并加载BERT模型\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        self.freezeSeed()\n",
    "        # 使用GPU，通过model.to(device)的方式使用\n",
    "        device_s = \"cuda:\" + cuda_num\n",
    "        self.device = torch.device(device_s if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        # import os\n",
    "        # result_dir = \"../result\"\n",
    "        # MODEL_PATH = self.config.get(\"BERT_path\", \"file_path\")\n",
    "        # config_PATH = self.config.get(\"BERT_path\", \"config_path\")\n",
    "        vocab_PATH = self.config.get(\"BERT_path\", \"vocab_path\")\n",
    "\n",
    "        # num_labels = self.config.get(\"training_rule\", \"num_labels\")\n",
    "        # hidden_dropout_prob = self.config.get(\"training_rule\", \"hidden_dropout_prob\")\n",
    "\n",
    "        # 通过词典导入分词器\n",
    "        self.tokenizer = transformers.BertTokenizer.from_pretrained(vocab_PATH)\n",
    "        # self.model_config = BertConfig.from_pretrained(config_PATH, num_labels=num_labels,\n",
    "        #                                                hidden_dropout_prob=hidden_dropout_prob)\n",
    "        # self.model = BertForSequenceClassification.from_pretrained(MODEL_PATH, config=self.model_config)\n",
    "        \"\"\"\n",
    "        train loss:  0.10704718510208534 \t train acc: 0.9637151849872321\n",
    "        valid loss:  0.17820182011222863 \t valid acc: 0.9459971577451445\n",
    "        \"\"\"\n",
    "        # 如果想换模型，换成下边这句子\n",
    "        # bert+lr 跟官方方法差不都\n",
    "        # self.model = bert_lr(bert_lr_Config())\n",
    "        # self.model = bert_cnn(bert_cnn_Config())\n",
    "        self.model = bert_lr_last4layer(bert_lr_last4layer_Config())\n",
    "\n",
    "        self.model.to(self.device)\n",
    "\n",
    "    def model_setup(self, zeng=0):\n",
    "        weight_decay = self.config.get(\"training_rule\", \"weight_decay\")\n",
    "        learning_rate = self.config.get(\"training_rule\", \"learning_rate\")\n",
    "        print(\"**model_setup:\")\n",
    "        print(\"zeng\",zeng)\n",
    "        if zeng == 1:\n",
    "            learning_rate = learning_rate * 2\n",
    "        # 定义优化器和损失函数\n",
    "        # Prepare optimizer and schedule (linear warmup and decay)\n",
    "        no_decay = ['bias', 'LayerNorm.weight']\n",
    "        optimizer_grouped_parameters = [\n",
    "            {'params': [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "             'weight_decay': weight_decay},\n",
    "            {'params': [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "             'weight_decay': 0.0}\n",
    "        ]\n",
    "        self.optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n",
    "        self.criterion = nn.CrossEntropyLoss(weight=torch.from_numpy(np.array(www)).float())\n",
    "        self.criterion.to(self.device)\n",
    "\n",
    "    def get_data(self):\n",
    "        \"\"\"\n",
    "        读取数据\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        train_set_path = self.config.get(\"data_path\", \"trainingSet_path\")\n",
    "        valid_set_path = self.config.get(\"data_path\", \"valSet_path\")\n",
    "        batch_size = self.config.get(\"training_rule\", \"batch_size\")\n",
    "        zeng_set_path = self.config.get(\"data_path\", \"zeng_path\")\n",
    "        bad_set_path=self.config.get(\"data_path\", \"bad_path\")\n",
    "        print(train_set_path,valid_set_path,batch_size,zeng_set_path,bad_set_path)\n",
    "\n",
    "        # 数据读入\n",
    "        # 加载数据集\n",
    "        sentiment_train_set = SentimentDataset(train_set_path)\n",
    "        sentiment_train_loader = DataLoader(sentiment_train_set, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "        sentiment_valid_set = SentimentDataset(valid_set_path)\n",
    "        sentiment_valid_loader = DataLoader(sentiment_valid_set, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "        sentiment_zeng_set = SentimentDataset(zeng_set_path)\n",
    "        sentiment_zeng_loader = DataLoader(sentiment_zeng_set, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "        \n",
    "        sentiment_bad_set = SentimentDataset(bad_set_path)\n",
    "        sentiment_bad_loader = DataLoader(sentiment_bad_set, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "        return sentiment_train_loader, sentiment_valid_loader, sentiment_zeng_loader,sentiment_bad_loader\n",
    "    \n",
    "    def get_bad_loss(self,iterator,loss_t=1):\n",
    "        epoch_acc = 0\n",
    "        \n",
    "        for _, batch in enumerate(iterator):\n",
    "            label = batch[\"label\"]\n",
    "            text = batch[\"text\"]\n",
    "#             print(text,label)\n",
    "            label=[int(i) for i in label]\n",
    "            label=torch.tensor(label)\n",
    "#             print(text,label)\n",
    "\n",
    "            input_ids, token_type_ids = convert_text_to_ids(self.tokenizer, text)\n",
    "            input_ids = seq_padding(self.tokenizer, input_ids)\n",
    "            token_type_ids = seq_padding(self.tokenizer, token_type_ids)\n",
    "            label = label.unsqueeze(1)\n",
    "            input_ids, token_type_ids, label = input_ids.long(), token_type_ids.long(), label.long()\n",
    "            input_ids, token_type_ids, label = input_ids.to(self.device), token_type_ids.to(self.device), label.to(\n",
    "                self.device)\n",
    "            output = self.model(input_ids=input_ids, token_type_ids=token_type_ids, labels=label)\n",
    "            # 更改了以下部分\n",
    "            # y_pred_label = output[1].argmax(dim=1)\n",
    "            y_pred_prob = output[1]\n",
    "            if loss_t==1:\n",
    "                loss=0\n",
    "                for i in range(len(y_pred_prob)):\n",
    "                    pre=y_pred_prob[i][label[i]]\n",
    "    #                 print(text[i])\n",
    "    #                 print(pre)\n",
    "\n",
    "                    loss+=pre\n",
    "\n",
    "                loss=loss/500\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                # 梯度清零\n",
    "                self.optimizer.zero_grad()\n",
    "            \n",
    "#             loss = output[0]\n",
    "            # loss = self.criterion(y_pred_prob.view(-1, 2), label.view(-1))\n",
    "            with torch.no_grad():\n",
    "                y_pred_label = y_pred_prob.argmax(dim=1)\n",
    "                acc = ((y_pred_label == label.view(-1)).sum()).item()\n",
    "                epoch_acc += acc\n",
    "\n",
    "            \n",
    "            \n",
    "\n",
    "        return epoch_acc / len(iterator.dataset.dataset)\n",
    "        \n",
    "        \n",
    "    def train_an_epoch(self, iterator,iterator_bad, zeng=0):\n",
    "        print(\"**train_an_epoch\")\n",
    "        print(\"zeng\",zeng)\n",
    "        self.model_setup(zeng)\n",
    "        epoch_loss = 0\n",
    "        epoch_acc = 0\n",
    "\n",
    "        for i, batch in enumerate(iterator):\n",
    "            label = batch[\"label\"]\n",
    "            text = batch[\"text\"]\n",
    "#             print(label)\n",
    "            input_ids, token_type_ids = convert_text_to_ids(self.tokenizer, text)\n",
    "            input_ids = seq_padding(self.tokenizer, input_ids)\n",
    "            token_type_ids = seq_padding(self.tokenizer, token_type_ids)\n",
    "            # 标签形状为 (batch_size, 1)\n",
    "            label = label.unsqueeze(1)\n",
    "            # 需要 LongTensor\n",
    "            input_ids, token_type_ids, label = input_ids.long(), token_type_ids.long(), label.long()\n",
    "            # 迁移到GPU\n",
    "            input_ids, token_type_ids, label = input_ids.to(self.device), token_type_ids.to(self.device), label.to(\n",
    "                self.device)\n",
    "            output = self.model(input_ids=input_ids, token_type_ids=token_type_ids, labels=label)  # 这里不需要labels\n",
    "            # BertForSequenceClassification的输出loss和logits\n",
    "            # BertModel原本的模型输出是last_hidden_state，pooler_output\n",
    "            # bert_cnn的输出是[batch_size, num_class]\n",
    "            # print(numpy.array(torch.tensor(output).cpu()).shape)\n",
    "\n",
    "            y_pred_prob = output[1]\n",
    "\n",
    "            # 计算loss\n",
    "            # 这个 loss 和 output[0] 是一样的\n",
    "            loss = self.criterion(y_pred_prob.view(-1, type_num), label.view(-1))  # 多分类改这里\n",
    "            loss.backward()\n",
    "            \n",
    "            self.optimizer.step()\n",
    "            # 梯度清零\n",
    "            self.optimizer.zero_grad()\n",
    "            \n",
    "            if i % 120 == 1:\n",
    "                print('bad_loss_backward')\n",
    "                bad_acc=self.get_bad_loss(iterator_bad)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                y_pred_label = y_pred_prob.argmax(dim=1)\n",
    "                # 计算acc\n",
    "                acc = ((y_pred_label == label.view(-1)).sum()).item()\n",
    "                # epoch 中的 loss 和 acc 累加\n",
    "                epoch_loss += loss.item()\n",
    "                epoch_acc += acc\n",
    "            if i % 120 == 0:\n",
    "                bad_acc=self.get_bad_loss(iterator_bad,0)\n",
    "                print('batch:',i,'/',len(iterator),\"\\tcurrent loss:\", epoch_loss / (i + 1), \"\\tcurrent acc:\", epoch_acc / ((i + 1) * len(label)),'\\tbad_acc:',bad_acc)\n",
    "        return epoch_loss / len(iterator), epoch_acc / len(iterator.dataset.dataset)\n",
    "\n",
    "    def evaluate(self, iterator,xian):\n",
    "        self.model.eval()\n",
    "        epoch_loss = 0\n",
    "        epoch_acc = 0\n",
    "        y_pred_label_all = []\n",
    "        label_all = []\n",
    "        with torch.no_grad():\n",
    "            for _, batch in enumerate(iterator):\n",
    "                label = batch[\"label\"]\n",
    "                text = batch[\"text\"]\n",
    "\n",
    "                input_ids, token_type_ids = convert_text_to_ids(self.tokenizer, text)\n",
    "                input_ids = seq_padding(self.tokenizer, input_ids)\n",
    "                token_type_ids = seq_padding(self.tokenizer, token_type_ids)\n",
    "                label = label.unsqueeze(1)\n",
    "                input_ids, token_type_ids, label = input_ids.long(), token_type_ids.long(), label.long()\n",
    "                input_ids, token_type_ids, label = input_ids.to(self.device), token_type_ids.to(self.device), label.to(\n",
    "                    self.device)\n",
    "                output = self.model(input_ids=input_ids, token_type_ids=token_type_ids, labels=label)\n",
    "                # 更改了以下部分\n",
    "                # y_pred_label = output[1].argmax(dim=1)\n",
    "                y_pred_prob = output[1]\n",
    "                \n",
    "                y_pred_label=[]\n",
    "                for i in y_pred_prob:\n",
    "                    if max(i)>xian:\n",
    "                        y_pred_label.append(i.argmax(dim=0))\n",
    "                    else:\n",
    "#                         print(i)\n",
    "#                         print(max(i))\n",
    "                        y_pred_label.append(torch.tensor(-1).to(self.device))\n",
    "#                         print(y_pred_label)\n",
    "#                 y_pred_label=np.array(y_pred_label)\n",
    "                y_pred_label=torch.tensor(y_pred_label).to(self.device)\n",
    "    \n",
    "                loss = output[0]\n",
    "                # loss = self.criterion(y_pred_prob.view(-1, 2), label.view(-1))\n",
    "                acc = ((y_pred_label == label.view(-1)).sum()).item()\n",
    "                y_pred_label_all += y_pred_label.tolist()\n",
    "                label_all += label.view(-1).tolist()\n",
    "\n",
    "                epoch_loss += loss.item()\n",
    "                epoch_acc += acc\n",
    "        target_names=['其他','ICT','新能源汽车','生物医药','医疗器械','钢铁','能源','工业机器人','先进轨道交通','数控机床','工业软件','高端装备','半导体','人工智能','稀土']\n",
    "        print(metrics.classification_report(label_all, y_pred_label_all,target_names=target_names))\n",
    "        print(\"准确率:\", metrics.accuracy_score(label_all, y_pred_label_all))\n",
    "        return epoch_loss / len(iterator), epoch_acc / len(iterator.dataset.dataset)\n",
    "    \n",
    "    def evval(self,xian):\n",
    "        sentiment_train_loader, sentiment_valid_loader, sentiment_zeng_loader ,sentiment_bad_loader= self.get_data()\n",
    "        valid_loss, valid_acc = self.evaluate(sentiment_valid_loader,xian)\n",
    "        print(\"valid loss: \", valid_loss, \"\\t\", \"valid acc:\", valid_acc)\n",
    "    \n",
    "    def evbad(self):\n",
    "        sentiment_train_loader, sentiment_valid_loader, sentiment_zeng_loader ,sentiment_bad_loader= self.get_data()\n",
    "        bad_acc=self.get_bad_loss(sentiment_bad_loader,0)\n",
    "        print(\"bad_acc: \", bad_acc)\n",
    "        \n",
    "    def train(self, epochs, zeng=0):\n",
    "        sentiment_train_loader, sentiment_valid_loader, sentiment_zeng_loader ,sentiment_bad_loader= self.get_data()\n",
    "        best_valid_loss=999\n",
    "        best_valid_acc=0\n",
    "        for i in range(epochs):\n",
    "            print('____________________________________________________________________________________')\n",
    "            print('____________________________________________________________________________________')\n",
    "            print('epochs:', i)\n",
    "            print('____________________________________________________________________________________')\n",
    "            print('____________________________________________________________________________________')\n",
    "            print('____train____')\n",
    "            if zeng == 0:\n",
    "                train_loss, train_acc = self.train_an_epoch(sentiment_train_loader,sentiment_bad_loader)\n",
    "            else:\n",
    "                train_loss, train_acc = self.train_an_epoch(sentiment_zeng_loader, 1)\n",
    "            print(\"train loss: \", train_loss, \"\\t\", \"train acc:\", train_acc)\n",
    "            print('____evaluate____')\n",
    "            valid_loss, valid_acc = self.evaluate(sentiment_valid_loader,0.97)\n",
    "            if valid_loss<best_valid_loss or valid_acc>best_valid_acc:\n",
    "                best_valid_loss=min(best_valid_loss,valid_loss)\n",
    "                best_valid_acc=max(best_valid_acc,valid_acc)\n",
    "                torch.save(classifier, model_save_path+'_'+str(i)+'_'+str(round(valid_loss,5))+'_'+str(round(valid_acc,5)))\n",
    "            print(\"valid loss: \", valid_loss, \"\\t\", \"valid acc:\", valid_acc)\n",
    "        # self.save_model()\n",
    "\n",
    "    def save_model(self):\n",
    "        model_save_path = self.config.get(\"result\", \"model_save_path\")\n",
    "        config_save_path = self.config.get(\"result\", \"config_save_path\")\n",
    "        vocab_save_path = self.config.get(\"result\", \"vocab_save_path\")\n",
    "\n",
    "        model_to_save = self.model.module if hasattr(self.model, 'module') else self.model\n",
    "        torch.save(model_to_save.state_dict(), model_save_path)\n",
    "        # model_to_save.config.to_json_file(config_save_path) # !!!'bert_lr' object has no attribute 'config'\n",
    "        # self.tokenizer.save_vocabulary(vocab_save_path)\n",
    "        print(\"model saved...\")\n",
    "\n",
    "    def predict(self, sentence):\n",
    "        # self.model.setup()\n",
    "        self.model_setup()\n",
    "        self.model.eval()\n",
    "        # 转token后padding\n",
    "        input_ids, token_type_ids = convert_text_to_ids(self.tokenizer, sentence)\n",
    "        input_ids = seq_padding(self.tokenizer, [input_ids])\n",
    "        token_type_ids = seq_padding(self.tokenizer, [token_type_ids])\n",
    "        # 需要 LongTensor\n",
    "        input_ids, token_type_ids = input_ids.long(), token_type_ids.long()\n",
    "        # 梯度清零\n",
    "        self.optimizer.zero_grad()\n",
    "        # 迁移到GPU\n",
    "        input_ids, token_type_ids = input_ids.to(self.device), token_type_ids.to(self.device)\n",
    "        output = self.model(input_ids=input_ids, token_type_ids=token_type_ids)\n",
    "        # y_pred_prob:各个类别的概率\n",
    "        y_pred_prob = output[0]\n",
    "#         y_pred_prob=self.sigmoid(y_pred_prob)\n",
    "        # 取概率最大的标签\n",
    "        y_pred_label = y_pred_prob.argmax(dim=1)\n",
    "\n",
    "        # 将torch.tensor转换回int形式\n",
    "        return y_pred_prob, y_pred_label.item()\n",
    "\n",
    "    def freezeSeed(self):\n",
    "        seed = 1\n",
    "        torch.manual_seed(seed)\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        np.random.seed(seed)  # Numpy module.\n",
    "        random.seed(seed)  # Python random module.\n",
    "        torch.manual_seed(seed)\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        \n",
    "\n",
    "def read_list(text_path):\n",
    "    lsit = []\n",
    "    with open('%s' % text_path, 'r', encoding=\"utf8\") as f:  # 打开一个文件只读模式\n",
    "        line = f.readlines()  # 读取文件中的每一行，放入line列表中\n",
    "        for line_list in line:\n",
    "            lsit.append(line_list.replace('\\n', ''))\n",
    "    return lsit\n",
    "\n",
    "\n",
    "def test(test_list,classifier,ty):\n",
    "    res=[]\n",
    "    for i in test_list:\n",
    "        re = classifier.predict(i)  # 0\n",
    "        result1 = re[1]\n",
    "        result2 = re[0].tolist()\n",
    "        \n",
    "        if result2[0][result1] < 0.997:\n",
    "            res.append('其他')\n",
    "            print(i, '\\n', result1, '***** 其他 ***** 原预测:',ty[result1], result2[0][result1], '\\n', result2[0], '\\n')\n",
    "        else:\n",
    "            res.append(ty[result1])\n",
    "            print(i, '\\n', result1, ty[result1], result2[0][result1], '\\n', result2[0], '\\n')\n",
    "        return res\n",
    "\n",
    "\n",
    "\n",
    "def run(list_):\n",
    "    device = torch.device(\"cpu\")\n",
    "    model_save_path='./classifier_12.13_8_1.88932_0.8347'\n",
    "    classifier= torch.load(model_save_path,map_location=device)\n",
    "    classifier.device=torch.device('cpu')\n",
    "    # print(classifier1.predict(\"『巴西』圣保罗城际铁路听证会延期至10月15日\"))  # 0\n",
    "    # print(classifier1.predict(\"永恒力叉车入驻京东工业品 载重2吨的叉车设备也能线上采购\"))  # 0\n",
    "    ty = ['ICT', '新能源汽车', '生物医药', '医疗器械', '钢铁', \n",
    "       '能源', '工业机器人', '先进轨道交通', '数控机床', '工业软件', \n",
    "       '高端装备', '半导体', '人工智能', '稀土']\n",
    "#     test_list = read_list('/data/fuwen/SuWen/Bert-classification/src/test.txt')\n",
    "    res=test(list_,classifier,ty) #主要是这句\n",
    "    \n",
    "# run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "model_save_path='/data/fuwen/SuWen/Bert-classification/result3/classifier_12.13_8_1.88932_0.8347'\n",
    "classifier= torch.load(model_save_path,map_location=device)\n",
    "classifier.device=torch.device('cpu')\n",
    "# print(classifier1.predict(\"『巴西』圣保罗城际铁路听证会延期至10月15日\"))  # 0\n",
    "# print(classifier1.predict(\"永恒力叉车入驻京东工业品 载重2吨的叉车设备也能线上采购\"))  # 0\n",
    "ty = ['ICT', '新能源汽车', '生物医药', '医疗器械', '钢铁', \n",
    "      '能源', '工业机器人', '先进轨道交通', '半导体', '高端设备', \n",
    "      '工业软件', '人工智能', '数控机床', '稀土']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_list=['香港“改革”后的选举日  伦敦港人同步“公投”不承认港府执政地位','星巴克私换标签使用过期食材 店内一天4种过期食材给顾客饮用']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**model_setup:\n",
      "zeng 0\n",
      "香港“改革”后的选举日  伦敦港人同步“公投”不承认港府执政地位 \n",
      " 10 ***** 其他 ***** 原预测: 高端装备 0.9820877909660339 \n",
      " [0.00031355724786408246, 0.001583037548698485, 0.00010986378038069233, 0.00018513339455239475, 3.542547347024083e-05, 0.0009698119829408824, 9.086172212846577e-05, 0.00013705072342418134, 0.0012023613089695573, 0.0005687929806299508, 0.9820877909660339, 0.0001872802822617814, 0.6087018847465515, 0.000895452219992876] \n",
      "\n",
      "**model_setup:\n",
      "zeng 0\n",
      "星巴克私换标签使用过期食材 店内一天4种过期食材给顾客饮用 \n",
      " 11 ***** 其他 ***** 原预测: 半导体 0.9967921376228333 \n",
      " [0.011564906686544418, 0.4510265290737152, 0.00022427967633120716, 0.0009386922465637326, 9.492277968092822e-06, 0.0010249731130897999, 0.002729119500145316, 0.0002032003685599193, 0.00037412019446492195, 0.0003386638418305665, 0.0006875005201436579, 0.9967921376228333, 0.007156172767281532, 3.468485738267191e-05] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "test(test_list,classifier,ty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sw0",
   "language": "python",
   "name": "sw0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
