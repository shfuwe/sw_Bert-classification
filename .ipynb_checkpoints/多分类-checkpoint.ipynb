{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_num=9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "手动实现transformer.models.bert.BertForSequenceClassification()函数\n",
    "根据论文[How to Fine-Tune BERT for Text Classification（2019）](https://www.aclweb.org/anthology/P18-1031.pdf)\n",
    "在分类问题上，把最后四层进行concat然后maxpooling 输出的结果会比直接输出最后一层的要好\n",
    "这里进行实现测试\n",
    "\n",
    "\"\"\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertModel,BertTokenizer\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class bert_lr_last4layer_Config(nn.Module):\n",
    "    def __init__(self):\n",
    "        self.bert_path = \"../chinese-bert-wwm\"\n",
    "        self.config_path = \"../chinese-bert-wwm/config.json\"\n",
    "\n",
    "        # self.tokenizer = BertTokenizer.from_pretrained(self.bert_path)\n",
    "        self.hidden_size = 768\n",
    "        self.num_labels = type_num\n",
    "        # self.dropout_bertout = 0.2\n",
    "        self.dropout_bertout = 0.5\n",
    "        self.mytrainedmodel = \"../result/bert_clf_model.bin\"\n",
    "        \"\"\"\n",
    "        current loss: 0.4363991916179657 \t current acc: 0.8125\n",
    "        current loss: 0.1328232882924341 \t current acc: 0.9527363184079602\n",
    "        current loss: 0.11797185830000853 \t current acc: 0.9585411471321695\n",
    "        train loss:  0.11880445411248554 \t train acc: 0.9583704495516361\n",
    "        valid loss:  0.1511497257672476 \t valid acc: 0.9431549028896258\n",
    "        \"\"\"\n",
    "\n",
    "class bert_lr_last4layer(nn.Module):\n",
    "\n",
    "    def __init__(self,config):\n",
    "        super(bert_lr_last4layer, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(config.bert_path,config = config.config_path)\n",
    "        self.dropout_bertout = nn.Dropout(config.dropout_bertout)\n",
    "        self.num_labels = config.num_labels\n",
    "        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
    "\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        labels=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=True,\n",
    "        return_dict=None,\n",
    "    ):\n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        # outputs = outputs[2] # [1]是pooled的结果 # [3]是hidden_states 12层\n",
    "        hidden_states = outputs.hidden_states\n",
    "        nopooled_output = torch.cat((hidden_states[9],hidden_states[10],hidden_states[11],hidden_states[12]),1)\n",
    "        batch_size = nopooled_output.shape[0] # 32\n",
    "        # print(batch_size)\n",
    "        # print(nopooled_output.shape) # torch.Size([32, 400, 768])\n",
    "        kernel_hight = nopooled_output.shape[1]\n",
    "        pooled_output = F.max_pool2d(nopooled_output,kernel_size = (kernel_hight,1))\n",
    "        # print(pooled_output.shape) # torch.Size([32, 1, 768])\n",
    "\n",
    "        flatten = pooled_output.view(batch_size,-1)\n",
    "        # print(flatten.shape) # [32,768]\n",
    "\n",
    "        flattened_output = self.dropout_bertout(flatten)\n",
    "\n",
    "        logits = self.classifier(flattened_output)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            if self.num_labels == 1:\n",
    "                #  We are doing regression\n",
    "                loss_fct = nn.MSELoss()\n",
    "                loss = loss_fct(logits.view(-1), labels.view(-1))\n",
    "            else:\n",
    "                loss_fct = nn.CrossEntropyLoss()\n",
    "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (logits,) + outputs[2:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return loss,logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config(object):\n",
    "    def __init__(self):\n",
    "        self.config_dict = {\n",
    "            \"data_path\": {\n",
    "                # \"trainingSet_path\": \"../data/sentiment/sentiment.train0.data\",\n",
    "                # \"valSet_path\": \"../data/sentiment/sentiment.valid0.data\",\n",
    "                \"trainingSet_path\": \"../data/sentiment/quan_9/train_10.14.txt\",\n",
    "                \"valSet_path\": \"../data/sentiment/quan_9/test_10.14.txt\",\n",
    "                \"testingSet_path\": \"../data/sentiment/sentiment.test0.data\",\n",
    "                \"zeng_path\": \"../data/sentiment/quan_9/zeng.txt\"\n",
    "            },\n",
    "\n",
    "            \"BERT_path\": {\n",
    "                \"file_path\": '../chinese-bert-wwm/',\n",
    "                \"config_path\": '../chinese-bert-wwm/',\n",
    "                \"vocab_path\": '../chinese-bert-wwm/',\n",
    "            },\n",
    "\n",
    "            \"training_rule\": {\n",
    "                \"max_length\": 300,  # 输入序列长度，别超过512\n",
    "                \"hidden_dropout_prob\": 0.3,\n",
    "                \"num_labels\": 9,  # 几分类个数\n",
    "                \"learning_rate\": 1e-5,\n",
    "                \"weight_decay\": 1e-2,\n",
    "                \"batch_size\": 16\n",
    "            },\n",
    "\n",
    "            \"result\": {\n",
    "                \"model_save_path\": '../result/bert_clf_model.bin',\n",
    "                \"config_save_path\": '../result/bert_clf_config.json',\n",
    "                \"vocab_save_path\": '../result/bert_clf_vocab.txt'\n",
    "            }\n",
    "        }\n",
    "\n",
    "    def get(self, section, name):\n",
    "        return self.config_dict[section][name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sw0",
   "language": "python",
   "name": "sw0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
